{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lalogonavy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"s3-redshift-data-migrate.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's install some requirements and set some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the HOST and DATA_BUCKET variables and add the values from the Learning Activity Credentials screen. Example values have been provided in order to help ensure you use the right ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOST = \"cfst-CHANGEME-redshiftcluster-otherjibberish.123123abcabc.us-east-1.redshift.amazonaws.com\" # Change this too\n",
    "# DATA_BUCKET = \"cfst-1279-9d31b9a9fc45a278465028065914d2-s3bucket-1vwpozq2bm4ss\" # Change this \n",
    "DATABASE = \"dev\"\n",
    "USER = \"clouduser\"\n",
    "PASSWORD = \"Fa%YrN^Pq4.xM\"\n",
    "PORT = 5439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will establish a connection to the database and test that connection by reading the database name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pprint import pprint\n",
    "\n",
    "query = '''SELECT datname FROM pg_database;'''\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=HOST,\n",
    "    user=USER,\n",
    "    port=PORT,\n",
    "    password=PASSWORD,\n",
    "    dbname=DATABASE\n",
    ")\n",
    "\n",
    "def runquery(conn,query,commit_bool=False):\n",
    "    \"\"\"\n",
    "    Just run a query given a connection\n",
    "    \"\"\"\n",
    "    \n",
    "    curr=conn.cursor()\n",
    "    curr.execute(query)\n",
    "    if commit_bool:\n",
    "        conn.commit()\n",
    "        return None\n",
    "    for row in curr.fetchall():\n",
    "        pprint(row)\n",
    "    return None\n",
    "\n",
    "runquery(conn, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we create a Redshift Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_create_query = '''\n",
    "create table movies(\n",
    "    title varchar(300) not null,\n",
    "    year integer not null,\n",
    "    rating real not null,\n",
    "    running_time_secs integer not null\n",
    ");\n",
    "'''\n",
    "\n",
    "runquery(conn, table_create_query, commit_bool=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's view the databases to ensure it was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_public_tables_query = '''\n",
    "SELECT DISTINCT\n",
    "  tablename\n",
    "FROM\n",
    "  PG_TABLE_DEF\n",
    "WHERE\n",
    "  schemaname = 'public';\n",
    "'''\n",
    "\n",
    "\n",
    "runquery(conn, view_public_tables_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to get our data into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('./data.csv', DATA_BUCKET, 'data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Data from S3 to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM_ROLE = \"arn:aws:iam::123456789123:role/cfst-1279-b8287a53b943ea3-CloudUserAndRedshiftIAMR-1DY76SW011SCJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_query = '''\n",
    "copy movies from 's3://{0}/data.csv'\n",
    "iam_role '{1}'\n",
    "CSV\n",
    "IGNOREHEADER 1;\n",
    "'''.format(DATA_BUCKET, IAM_ROLE)\n",
    "\n",
    "\n",
    "runquery(conn, view_public_tables_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add a sample record and SELECT from our database to ensure the database is setup properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"INSERT INTO \" + tablename + \" (Year, Title, Actor, Rating, Runtime, Uploaded) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "val = (\"2018\", \"The DynamoDB Movie\", \"Derek Morgan\", \"10\", \"7920\", \"yes\")\n",
    "mycursor.execute(sql, val)\n",
    "\n",
    "mydb.commit()\n",
    "\n",
    "print(mycursor.rowcount, \"record inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb.commit()\n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"SELECT * FROM \" + tablename)\n",
    "\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "for x in myresult:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have setup the relational database, let's create our \"Movies\" DynamoDB table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "dynamodb = boto3.resource('dynamodb',  region_name='us-east-1')\n",
    "\n",
    "\n",
    "table = dynamodb.create_table(\n",
    "   TableName='Movies',\n",
    "   KeySchema=[\n",
    "       {\n",
    "           'AttributeName': 'year',\n",
    "           'KeyType': 'HASH'  #Partition key\n",
    "       },\n",
    "       {\n",
    "           'AttributeName': 'title',\n",
    "           'KeyType': 'RANGE'  #Sort key\n",
    "       }\n",
    "   ],\n",
    "   AttributeDefinitions=[\n",
    "       {\n",
    "           'AttributeName': 'year',\n",
    "           'AttributeType': 'N'\n",
    "       },\n",
    "       {\n",
    "           'AttributeName': 'title',\n",
    "           'AttributeType': 'S'\n",
    "       },\n",
    "\n",
    "   ],\n",
    "   ProvisionedThroughput={\n",
    "       'ReadCapacityUnits': 2,\n",
    "       'WriteCapacityUnits': 2\n",
    "   }\n",
    ")\n",
    "\n",
    "# Wait until the table exists.\n",
    "table.meta.client.get_waiter('table_exists').wait(TableName='Movies')\n",
    "print('Table is ready, please continue as instructed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's create our Lambda function. This will process the DynamoDB Stream and insert the records into our relational database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('lambda',  region_name='us-east-1')\n",
    "\n",
    "\n",
    "response = client.create_function(\n",
    "    FunctionName='ddbStream',\n",
    "    Runtime='nodejs4.3',\n",
    "    Role= lambdaRole,\n",
    "    Handler='ddbStream.handler',\n",
    "    Code={\n",
    "        'ZipFile': open('ddbStream.zip', 'rb').read()\n",
    "    },\n",
    "    Description='Extracts from DynamoDB Stream and adds to Relational DB',\n",
    "    Timeout=5,\n",
    "    Environment={\n",
    "        'Variables': {\n",
    "            'endPoint': endpoint\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the prerequisites are setup, continue to the AWS console to configure the DynamoDB Stream and Trigger.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add two test items to initialize the function before we upload a lot more data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "dynamodb = boto3.resource('dynamodb',  region_name='us-east-1')\n",
    "table = dynamodb.Table('Movies')\n",
    "table.put_item(\n",
    "    Item={\n",
    "        \"year\": 2018,\n",
    "        \"title\": \"DynamoDB Streams\",\n",
    "        \"actor\": \"Derek Morgan\",\n",
    "        \"rating\": 10,\n",
    "        \"running_time\": 1800,\n",
    "        \"uploaded\": \"yes\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "dynamodb = boto3.resource('dynamodb',  region_name='us-east-1')\n",
    "table = dynamodb.Table('Movies')\n",
    "table.put_item(\n",
    "    Item={\n",
    "        \"year\": 2019,\n",
    "        \"title\": \"DynamoDB Streams 2: The Streamening\",\n",
    "        \"actor\": \"Derek Morgan\",\n",
    "        \"rating\": 10,\n",
    "        \"running_time\": 7200,\n",
    "        \"uploaded\": \"yes\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's check the relational database to ensure the data streamed. If it didn't, we can modify the data above and try again to ensure it's active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mydb.commit()\n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"SELECT * FROM \" + tablename +\";\")\n",
    "\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "item_list = []\n",
    "for i in myresult:\n",
    "    item = {'id':i[6], \n",
    "            'title':i[1], \n",
    "            'actor' :i[2], \n",
    "            'rating' :i[3], \n",
    "            'running_time' :i[4], \n",
    "            'uploaded' :i[5],\n",
    "            'year' :i[0]}\n",
    "    item_list.append(item)\n",
    "df = pd.DataFrame(data=item_list,columns=['id','year','title','actor','running_time','rating','uploaded'])\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's add 100 records to our DynamoDB table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Python 2/3 compatibility\n",
    "import boto3\n",
    "import json\n",
    "import decimal\n",
    "import sys\n",
    "import random\n",
    "dynamodb = boto3.resource('dynamodb',  region_name='us-east-1')\n",
    "table = dynamodb.Table('Movies')\n",
    "\n",
    "choices = ['yes', 'no']\n",
    "i = 0\n",
    "with open(\"moviedata.json\") as json_file:\n",
    "    with table.batch_writer() as batch:\n",
    "        movies = json.load(json_file, parse_float = decimal.Decimal)\n",
    "        for movie in movies:\n",
    "            i = i + 1\n",
    "            if i == 101:\n",
    "                break\n",
    "            year = int(movie['year'])\n",
    "            title = movie['title']\n",
    "            star = movie['actors'][0]\n",
    "            rating = movie['rating']\n",
    "            running_time = movie['running_time_secs']\n",
    "            uploaded = random.choice(choices)\n",
    "\n",
    "            print(\"Adding movie:\", year, title, star, rating, running_time, uploaded)\n",
    "\n",
    "            batch.put_item(\n",
    "               Item={\n",
    "                   'year': year,\n",
    "                   'title': title,\n",
    "                   'actor': star,\n",
    "                   'rating': rating,\n",
    "                   'running_time': running_time,\n",
    "                   'uploaded' : uploaded\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's find all items in our relational database where the rating was higher than '6' and uploaded is equal to 'no':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mydb.commit()\n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"SELECT * FROM \" + tablename + \" WHERE uploaded = 'no' AND rating > 6;\")\n",
    "\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "item_list = []\n",
    "for i in myresult:\n",
    "    item = {'id':i[6], \n",
    "            'title':i[1], \n",
    "            'actor' :i[2], \n",
    "            'rating' :i[3], \n",
    "            'running_time' :i[4], \n",
    "            'uploaded' :i[5],\n",
    "            'year' :i[0]}\n",
    "    item_list.append(item)\n",
    "df = pd.DataFrame(data=item_list,columns=['id','year','title','actor','running_time','rating','uploaded'])\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "The snippets below can help reset things to default or troubleshoot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete all items in the relational database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollback_query = '''rollback;'''\n",
    "runquery(conn, rollback_query, commit_bool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Done! Awesome Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
